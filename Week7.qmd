---
title: "Week7 Classification"
---

## Summary

### Mind Map

This is the mind map for Lecture 7. This week's lecture introduced how classied data is used and how to classify.

```{r echo=FALSE, out.width = "80%", fig.align='center', cache=FALSE}
knitr::include_graphics('images/week7pic/mindmap7.png') 
```

::: {align="center" style="font-size: smaller; color: gray;"}
<em>Figure: Mind map for Lecture 7</em>
:::

## Applications

SVM and ML classifiers have been extensively utilized for their robustness in various contexts. ML, often paired with supervised classification methods, has been applied in urban land cover and development mapping, achieving up to 84.4% overall accuracy in some studies [@thapa2009examining]. On the other hand, SVM, a set of related learning algorithms, has demonstrated overall accuracies surpassing 86.6% [@taati2015land] showcasing its superiority in producing high-quality results.

Accuracy assessment is a complex yet crucial step in land cover classification and mapping, involving the analysis of a map or classification's correctness [@foody2002status]. It includes measuring map quality, evaluating classification algorithms, and identifying errors. Our research suggests SVM as a preferable choice for precise land cover classification, especially in urban and built-up areas, due to its higher user and producer accuracy compared to ML. This recommendation is based on its effectiveness in distinguishing urban/built-up land cover, attributed to the more distinct features identified by SVM.

In the study by Rimal,Rijal and Kunwar, they focused on comparing the effectiveness of two widely used classification algorithms: Support Vector Machine (SVM) and Maximum Likelihood (ML), for land cover classification in the Kathmandu Valley of Nepal from 1988 to 2016. The research found that the User's and Producer's Accuracy of SVM classifier were both relatively higher than the ML classifier. And SVM classifier identified all the classes more accurately than the ML classifier[@rimal2020comparing].

```{r echo=FALSE, out.width = "100%", fig.align='center', cache=FALSE}
knitr::include_graphics('images/week7pic/application1.jpg') 
```

::: {align="center" style="font-size: smaller; color: gray;"}
<em>Figure: Overall classification accuracy of SVM and ML in different classification years</em>
:::

::: {align="center" style="font-size: smaller; color: gray;"}
<em>Source: [@rimal2020comparing]</em>
:::

## Reflections

This week's exploration into the image classification has been a blend of absorbing theory and practical learning. I'm intrigued by the concept of Gini impurity used in Classification and Regression Trees (CART), a straightforward yet powerful measure for decision-making in tree algorithms. Another highlight was grappling with the challenge of overfitting---a reminder that more complex models aren't always better, especially when they might not generalize well to new data.

Actually I am also studying these concepts in the module of CASA0006. I consider it's very useful and interesting to see how these methods are used in remote sensing.

As a graduate student, these insights are more than academic concepts; they're tools that can be wielded to uncover patterns in environmental data that are otherwise invisible. This knowledge empowers us to make informed decisions, whether it's in managing natural resources or planning urban expansions.

## References
